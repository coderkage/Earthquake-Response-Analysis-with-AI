{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_keywords = [\n",
    "    # Core Earthquake Terms\n",
    "    \"earthquake\", \"tremor\", \"aftershock\", \"seismic\", \"fault\", \"epicenter\",\n",
    "    \"magnitude\", \"Richter scale\", \"shaking\", \"ground\", \"quake\", \"foreshock\",\n",
    "    \"tectonic\", \"plate\", \"shockwave\", \"aftermath\", \"felt\", \"feel\",\n",
    "    \n",
    "    # Descriptive Terms\n",
    "    \"strong\", \"massive\", \"devastating\", \"violent\", \"powerful\", \"intense\",\n",
    "    \"mild\", \"deep\", \"surface\", \"shallow\",\n",
    "    \n",
    "    # Damages and Effects\n",
    "    \"damage\", \"collapse\", \"ruins\", \"wreckage\", \"destroyed\", \"cracks\",\n",
    "    \"crumbling\", \"aftermath\", \"impact\", \"destruction\", \"disaster\", \"displaced\",\n",
    "    \"homeless\", \"injury\", \"injuries\", \"fatalities\", \"debris\", \"rubble\", \"casualties\",\n",
    "    \"trapped\", \"death\", \"die\", \"died\", \"wreckage\",\n",
    "    \n",
    "    # Responses and Warnings\n",
    "    \"alert\", \"warning\", \"evacuation\", \"rescue\", \"search\",\n",
    "    \"emergency\", \"relief\", \"assistance\", \"volunteers\", \"preparedness\",\n",
    "    \"shelter\", \"relief efforts\", \"response team\",\n",
    "    \n",
    "    # Measurement and Science\n",
    "    \"Richter\", \"seismograph\", \"seismology\", \"intensity\", \"measurement\", \"scale\",\n",
    "    \"USGS\", \"depth\", \"geological\", \"seismometer\", \"seismic\",\n",
    "    \n",
    "    # Related Natural Disasters and Events\n",
    "    \"tsunami\", \"landslide\", \"fire\", \"eruption\", \"volcano\", \"flood\",\n",
    "    \n",
    "    # Social and Emotional Terms\n",
    "    \"pray\", \"thoughts\", \"fear\", \"panic\", \"trauma\", \"loss\", \"tragedy\",\n",
    "    \"devastation\", \"solidarity\", \"support\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.read_csv(\"preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Setting 'attr' to LOWER for case-insensitive matching\n",
    "\n",
    "\n",
    "# Load GeoNames data and create GPE patterns (as shown previously)\n",
    "countries = ['JP']\n",
    "gpe_phrases = []\n",
    "\n",
    "for country in countries:\n",
    "    file_path = f\"{country}.txt\"\n",
    "    data = pd.read_csv(file_path, sep='\\t', header=None, usecols=[2])  # Column 1 has place names\n",
    "    place_names = data[2].unique().tolist()\n",
    "    gpe_phrases.extend(place_names)\n",
    "\n",
    "# Remove duplicates from GPE phrases\n",
    "gpe_phrases = list(set(gpe_phrases))\n",
    "\n",
    "# Add additional phrases\n",
    "gpe_phrases.extend([\"Japan\"]) \n",
    "\n",
    "# Filter out non-string entries from gpe_phrases\n",
    "gpe_phrases = [phrase for phrase in gpe_phrases if isinstance(phrase, str)]\n",
    "gpe_patterns = [nlp.make_doc(phrase.lower()) for phrase in gpe_phrases]  # Lowercase GPE patterns\n",
    "matcher.add(\"GPE\", gpe_patterns)\n",
    "\n",
    "# Lowercase disaster patterns\n",
    "# disaster_patterns = [nlp.make_doc(keyword.lower()) for keyword in disaster_keywords]\n",
    "# matcher.add(\"DISASTER\", disaster_patterns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:08<00:00, 53.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List to store annotated data\n",
    "annotated_data = []\n",
    "\n",
    "# Process each text with tqdm progress bar\n",
    "for text in tqdm(merged_df['text'], total=len(merged_df)):\n",
    "    original_text = text  # Save original text\n",
    "    lowered_text = text.lower()  # Convert text to lowercase for matching\n",
    "    doc = nlp(lowered_text)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    entities = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        original_start = span.start_char\n",
    "        original_end = span.end_char\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "        entities.append([original_start, original_end, label])  # Use positions in original text\n",
    "\n",
    "    # Remove overlapping entities, keeping the shortest one\n",
    "    entities = sorted(entities, key=lambda x: (x[0], x[1] - x[0]))  # Sort by start position, then by span length\n",
    "    filtered_entities = []\n",
    "    last_end = -1\n",
    "    \n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # No overlap with the previous entity\n",
    "            filtered_entities.append([start, end, label])\n",
    "            last_end = end  # Update last_end to the end of the current entity\n",
    "\n",
    "    # Append the annotated entry with the original sentence\n",
    "    annotated_data.append([original_text, {\"entities\": filtered_entities}])\n",
    "\n",
    "# Save annotated data in JSONL format\n",
    "with open(\"FINAL_TEST1.jsonl\", \"w\") as file:\n",
    "    for entry in annotated_data:\n",
    "        json.dump(entry, file)\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete. Negative start index entities removed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to input JSONL file and output file\n",
    "input_file = \"FINAL_TEST.jsonl\"\n",
    "output_file = \"FINAL_TEST.jsonl\"\n",
    "\n",
    "# List to store cleaned data\n",
    "cleaned_data = []\n",
    "\n",
    "# Open and process each line in the JSONL file\n",
    "with open(input_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Load JSON data for each line\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Filter out entities with a negative start index\n",
    "        cleaned_entities = [entity for entity in data[1][\"entities\"] if entity[0] >= 0]\n",
    "        \n",
    "        # Update the data with cleaned entities\n",
    "        data[1][\"entities\"] = cleaned_entities\n",
    "        \n",
    "        # Append to cleaned_data list\n",
    "        cleaned_data.append(data)\n",
    "\n",
    "# Write cleaned data to a new JSONL file\n",
    "with open(output_file, \"w\") as file:\n",
    "    for entry in cleaned_data:\n",
    "        json.dump(entry, file)\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Cleanup complete. Negative start index entities removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
