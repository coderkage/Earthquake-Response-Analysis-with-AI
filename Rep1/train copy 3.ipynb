{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from thinc.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the NER pipeline if it's not present\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add your custom labels to the NER component\n",
    "labels = [\"GPE\", \"DISASTER\"]\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Function to load data from the .jsonl file\n",
    "def load_training_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    training_data = []\n",
    "    for line in lines:\n",
    "        entry = json.loads(line)\n",
    "        text = entry[0]\n",
    "        entities = entry[1][\"entities\"]\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "# Convert training data into spaCy's Example objects\n",
    "def create_training_examples(nlp, train_data):\n",
    "    examples = []\n",
    "    for text, annotations in tqdm(train_data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29914/29914 [00:26<00:00, 1121.99it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your custom NER training data\n",
    "train_data = load_training_data(\"train/data3.jsonl\")\n",
    "\n",
    "# Convert training data to spaCy's Example format\n",
    "train_examples = create_training_examples(nlp, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 38328.76468920891}\n",
      "Iteration 2, Losses: {'ner': 18232.913023904384}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     batches \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mminibatch(train_examples, size\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mcompounding(\u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m32.0\u001b[39m, \u001b[38;5;241m1.5\u001b[39m))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Update the model with a custom learning rate\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Save the updated model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\language.py:1201\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sgd \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1196\u001b[0m         name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc, ty\u001b[38;5;241m.\u001b[39mTrainableComponent)\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mis_trainable\n\u001b[0;32m   1199\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1200\u001b[0m     ):\n\u001b[1;32m-> 1201\u001b[0m         \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43msgd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m annotates:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc, eg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m   1204\u001b[0m         _pipe(\n\u001b[0;32m   1205\u001b[0m             (eg\u001b[38;5;241m.\u001b[39mpredicted \u001b[38;5;28;01mfor\u001b[39;00m eg \u001b[38;5;129;01min\u001b[39;00m examples),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         examples,\n\u001b[0;32m   1212\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:252\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.finish_update\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\model.py:346\u001b[0m, in \u001b[0;36mModel.finish_update\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mparam_names:\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mhas_grad(name):\n\u001b[1;32m--> 346\u001b[0m         param, grad \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m         node\u001b[38;5;241m.\u001b[39mset_param(name, param)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00001  # Change this to your desired value\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.resume_training()\n",
    "    nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "    # Initialize the optimizer with the NER component\n",
    "    # optimizer = nlp.create_optimizer()  # Just create the optimizer without any parameters\n",
    "    # nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "    # Define your custom learning rate and batch size\n",
    "\n",
    "    # Fine-tune the model on the new dataset\n",
    "    for iteration in range(30):  # Adjust the number of iterations as needed\n",
    "        losses = {}\n",
    "        # Create batches with the specified batch size\n",
    "        # batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "        batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.5))\n",
    "        for batch in batches:\n",
    "            # Update the model with a custom learning rate\n",
    "            nlp.update(batch, sgd=optimizer, drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {iteration + 1}, Losses: {losses}\")\n",
    "\n",
    "    # Save the updated model\n",
    "nlp.to_disk(\"0fner_model_custom_4\")\n",
    "\n",
    "print(\"Model fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aftershocks DISASTER\n",
      "M. Noto GPE\n",
      "earthquake DISASTER\n",
      "Japan GPE\n",
      "epicenter DISASTER\n",
      "Ishikawa GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "nlp_custom = spacy.load(\"0fner_model_custom_2_2\")\n",
    "\n",
    "# Test the model with a sample tweet\n",
    "doc = nlp_custom(\"The aftershocks pattern in purple suggests that the M. Noto earthquake ruptured bilaterally, with the preliminary NEIC slip model in Japan also indicating slip on both sides of the epicenter in Ishikawa\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
