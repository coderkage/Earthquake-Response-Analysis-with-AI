{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from thinc.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the NER pipeline if it's not present\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add your custom labels to the NER component\n",
    "labels = [\"GPE\", \"DISASTER\"]\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Function to load data from the .jsonl file\n",
    "def load_training_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    training_data = []\n",
    "    for line in lines:\n",
    "        entry = json.loads(line)\n",
    "        text = entry[0]\n",
    "        entities = entry[1][\"entities\"]\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "# Convert training data into spaCy's Example objects\n",
    "def create_training_examples(nlp, train_data):\n",
    "    examples = []\n",
    "    for text, annotations in tqdm(train_data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 989.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your custom NER training data\n",
    "train_data = load_training_data(\"train/data3.jsonl\")\n",
    "\n",
    "# Convert training data to spaCy's Example format\n",
    "train_examples = create_training_examples(nlp, train_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 1692.1467939288493}\n",
      "Iteration 2, Losses: {'ner': 499.4314064113034}\n",
      "Iteration 3, Losses: {'ner': 286.63393818879007}\n",
      "Iteration 4, Losses: {'ner': 197.08769977499125}\n",
      "Iteration 5, Losses: {'ner': 148.9309282132022}\n",
      "Iteration 6, Losses: {'ner': 85.0569526385531}\n",
      "Iteration 7, Losses: {'ner': 57.29338382675076}\n",
      "Iteration 8, Losses: {'ner': 73.48997945942445}\n",
      "Iteration 9, Losses: {'ner': 80.21941100280584}\n",
      "Iteration 10, Losses: {'ner': 59.791510166100345}\n",
      "Iteration 11, Losses: {'ner': 45.95952556672817}\n",
      "Iteration 12, Losses: {'ner': 49.282037949899}\n",
      "Iteration 13, Losses: {'ner': 24.695770303232806}\n",
      "Iteration 14, Losses: {'ner': 45.725618190981244}\n",
      "Iteration 15, Losses: {'ner': 36.029180169302485}\n",
      "Iteration 16, Losses: {'ner': 29.08204286306493}\n",
      "Iteration 17, Losses: {'ner': 43.84203517078039}\n",
      "Iteration 18, Losses: {'ner': 31.27555294935127}\n",
      "Iteration 19, Losses: {'ner': 34.76315409491644}\n",
      "Iteration 20, Losses: {'ner': 25.666622567994725}\n",
      "Model fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001  # Change this to your desired value\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.resume_training()\n",
    "    nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "    # Initialize the optimizer with the NER component\n",
    "    # optimizer = nlp.create_optimizer()  # Just create the optimizer without any parameters\n",
    "    # nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "    # Define your custom learning rate and batch size\n",
    "\n",
    "    # Fine-tune the model on the new dataset\n",
    "    for iteration in range(20):  # Adjust the number of iterations as needed\n",
    "        losses = {}\n",
    "        # Create batches with the specified batch size\n",
    "        # batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "        batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            # Update the model with a custom learning rate\n",
    "            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(f\"Iteration {iteration + 1}, Losses: {losses}\")\n",
    "\n",
    "    # Save the updated model\n",
    "nlp.to_disk(\"0fner_model_custom_4_1\")\n",
    "\n",
    "print(\"Model fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aftershocks DISASTER\n",
      "M. Noto GPE\n",
      "earthquake DISASTER\n",
      "Japan GPE\n",
      "epicenter DISASTER\n",
      "Ishikawa GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "nlp_custom = spacy.load(\"0fner_model_custom_2_2\")\n",
    "\n",
    "# Test the model with a sample tweet\n",
    "doc = nlp_custom(\"The aftershocks pattern in purple suggests that the M. Noto earthquake ruptured bilaterally, with the preliminary NEIC slip model in Japan also indicating slip on both sides of the epicenter in Ishikawa\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
