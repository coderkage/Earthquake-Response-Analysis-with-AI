{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from thinc.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the NER pipeline if it's not present\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add your custom labels to the NER component\n",
    "labels = [\"GPE\", \"DISASTER\"]\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Function to load data from the .jsonl file\n",
    "def load_training_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    training_data = []\n",
    "    for line in lines:\n",
    "        entry = json.loads(line)\n",
    "        text = entry[0]\n",
    "        entities = entry[1][\"entities\"]\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "# Convert training data into spaCy's Example objects\n",
    "def create_training_examples(nlp, train_data):\n",
    "    examples = []\n",
    "    for text, annotations in tqdm(train_data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to remove duplicate entities\n",
    "def remove_duplicate_entities(records):\n",
    "    for record in records:\n",
    "        text, annotations = record\n",
    "        unique_entities = []\n",
    "        seen = set()\n",
    "        for entity in annotations['entities']:\n",
    "            entity_tuple = tuple(entity)  # Convert the list to a tuple so it can be added to the set\n",
    "            if entity_tuple not in seen:\n",
    "                unique_entities.append(entity)\n",
    "                seen.add(entity_tuple)\n",
    "        annotations['entities'] = unique_entities\n",
    "    return records\n",
    "\n",
    "# Function to read data from a .jsonl file\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Function to load and clean test data\n",
    "def load_and_clean_test_data(file_path):\n",
    "    data = read_jsonl(file_path)\n",
    "    cleaned_data = remove_duplicate_entities(data)\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your custom NER training data\n",
    "# train_data = load_training_data(\"train/train_data_1.jsonl\")\n",
    "train_data = load_and_clean_test_data(\"train/filtered_annotated_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29914/29914 [00:06<00:00, 4426.29it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert training data to spaCy's Example format\n",
    "train_examples = create_training_examples(nlp, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 8470.175394881833}\n",
      "Iteration 2, Losses: {'ner': 3331.7147140198053}\n",
      "Iteration 3, Losses: {'ner': 2330.8960888534557}\n",
      "Iteration 4, Losses: {'ner': 1870.4056896015618}\n",
      "Iteration 5, Losses: {'ner': 1528.3078657274255}\n",
      "Iteration 6, Losses: {'ner': 1301.054835377876}\n",
      "Iteration 7, Losses: {'ner': 1069.2630843802335}\n",
      "Iteration 8, Losses: {'ner': 896.4830350697554}\n",
      "Iteration 9, Losses: {'ner': 767.0087452896743}\n",
      "Iteration 10, Losses: {'ner': 645.3387696542171}\n",
      "Iteration 11, Losses: {'ner': 625.5262262065476}\n",
      "Iteration 12, Losses: {'ner': 548.6241726491025}\n",
      "Iteration 13, Losses: {'ner': 569.2077308888863}\n",
      "Iteration 14, Losses: {'ner': 490.5553421924499}\n",
      "Iteration 15, Losses: {'ner': 414.4696965641796}\n",
      "Iteration 16, Losses: {'ner': 366.9997418613033}\n",
      "Iteration 17, Losses: {'ner': 379.5542888001005}\n",
      "Iteration 18, Losses: {'ner': 391.9755414983169}\n",
      "Iteration 19, Losses: {'ner': 418.2535545411854}\n",
      "Iteration 20, Losses: {'ner': 336.04969875900775}\n",
      "Model fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001  # Change this to your desired value\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.resume_training()\n",
    "    nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "    # Initialize the optimizer with the NER component\n",
    "    # optimizer = nlp.create_optimizer()  # Just create the optimizer without any parameters\n",
    "    # nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "    # Define your custom learning rate and batch size\n",
    "\n",
    "    # Fine-tune the model on the new dataset\n",
    "    for iteration in range(20):  # Adjust the number of iterations as needed\n",
    "        losses = {}\n",
    "        # Create batches with the specified batch size\n",
    "        batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            # Update the model with a custom learning rate\n",
    "            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(f\"Iteration {iteration + 1}, Losses: {losses}\")\n",
    "\n",
    "    # Save the updated model\n",
    "nlp.to_disk(\"0fner_model_custom_3_1\")\n",
    "\n",
    "print(\"Model fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M. Noto GPE\n",
      "earthquake DISASTER\n",
      "NEIC DISASTER\n",
      "epicenter DISASTER\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "nlp_custom = spacy.load(\"ner_model_custom\")\n",
    "\n",
    "# Test the model with a sample tweet\n",
    "doc = nlp_custom(\"The aftershock pattern in purple suggests that the M. Noto  earthquake ruptured bilaterally, with the preliminary NEIC slip model also indicating slip on both sides of the epicenter\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
