{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load SpaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to convert IOB2 tagged sentence to SpaCy's format\n",
    "def convert_iob_to_spacy_format(sentence, tags):\n",
    "    entities = []\n",
    "    start = None\n",
    "    for i, (word, tag) in enumerate(zip(sentence.split(), tags)):\n",
    "        if tag.startswith(\"B-\"):  # Beginning of an entity\n",
    "            if start is not None:  # Close previous entity\n",
    "                entities.append((start, i, entity_label))  # (start, end, label)\n",
    "            start = i\n",
    "            entity_label = tag[2:]  # Extract entity label\n",
    "        elif tag.startswith(\"I-\") and start is not None:\n",
    "            continue  # Inside an entity\n",
    "        else:  # Not an entity\n",
    "            if start is not None:  # Close previous entity\n",
    "                entities.append((start, i, entity_label))\n",
    "                start = None\n",
    "    # Close last entity if exists\n",
    "    if start is not None:\n",
    "        entities.append((start, len(sentence.split()), entity_label))\n",
    "\n",
    "    return {\"entities\": entities}\n",
    "\n",
    "# Function to evaluate NER performance\n",
    "def evaluate_ner(sentence, tags):\n",
    "    # Convert IOB tags to SpaCy format\n",
    "    annotations = convert_iob_to_spacy_format(sentence, tags)\n",
    "    example = Example.from_dict(nlp.make_doc(sentence), annotations)\n",
    "\n",
    "    # Get predictions\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract true and predicted entities\n",
    "    true_entities = set((start, end, label) for start, end, label in annotations[\"entities\"])\n",
    "    predicted_entities = set((ent.start_char, ent.end_char, ent.label_) for ent in doc.ents)\n",
    "\n",
    "    # Entity level evaluation\n",
    "    tp = len(true_entities & predicted_entities)  # True positives\n",
    "    fp = len(predicted_entities - true_entities)  # False positives\n",
    "    fn = len(true_entities - predicted_entities)  # False negatives\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    print(f\"Entity-level scores: Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Token level evaluation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for token in doc:\n",
    "        label = \"O\"  # Default to outside\n",
    "        for start, end, entity_label in annotations[\"entities\"]:\n",
    "            if start <= token.i < end:\n",
    "                label = \"B-\" + entity_label if token.i == start else \"I-\" + entity_label\n",
    "                break\n",
    "        y_true.append(label)\n",
    "        y_pred.append(token.ent_iob_ + '-' + token.ent_type_)\n",
    "\n",
    "    # Calculate token-level metrics\n",
    "    precision_token = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_token = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_token = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Token-level scores: Precision: {precision_token:.4f}, Recall: {recall_token:.4f}, F1-Score: {f1_token:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-level scores: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Token-level scores: Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-GPE       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepp\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"India\" with entities \"[(0, 1, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example IOB2 tagged sentence\n",
    "sentence = \"India\"\n",
    "tags = [\"B-GPE\"]\n",
    "\n",
    "# Evaluate NER\n",
    "evaluate_ner(sentence, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
