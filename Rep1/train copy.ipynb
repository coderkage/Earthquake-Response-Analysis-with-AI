{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from thinc.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the NER pipeline if it's not present\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add your custom labels to the NER component\n",
    "labels = [\"GPE\", \"DISASTER\"]\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Function to load data from the .jsonl file\n",
    "def load_training_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    training_data = []\n",
    "    for line in lines:\n",
    "        entry = json.loads(line)\n",
    "        text = entry[0]\n",
    "        entities = entry[1][\"entities\"]\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "# Convert training data into spaCy's Example objects\n",
    "def create_training_examples(nlp, train_data):\n",
    "    examples = []\n",
    "    for text, annotations in tqdm(train_data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1276.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your custom NER training data\n",
    "train_data = load_training_data(\"train/data3.jsonl\")\n",
    "\n",
    "# Convert training data to spaCy's Example format\n",
    "train_examples = create_training_examples(nlp, train_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 1568.1188160713702}\n",
      "Iteration 2, Losses: {'ner': 459.29892484944276}\n",
      "Iteration 3, Losses: {'ner': 252.24542299692504}\n",
      "Iteration 4, Losses: {'ner': 138.23971964243464}\n",
      "Iteration 5, Losses: {'ner': 90.47280653886523}\n",
      "Iteration 6, Losses: {'ner': 62.04907696036728}\n",
      "Iteration 7, Losses: {'ner': 62.68806317947039}\n",
      "Iteration 8, Losses: {'ner': 42.515644518425944}\n",
      "Iteration 9, Losses: {'ner': 34.906144071241776}\n",
      "Iteration 10, Losses: {'ner': 44.153884356830304}\n",
      "Iteration 11, Losses: {'ner': 41.33845888993389}\n",
      "Iteration 12, Losses: {'ner': 37.745458430482884}\n",
      "Iteration 13, Losses: {'ner': 46.708330930778345}\n",
      "Iteration 14, Losses: {'ner': 27.025751817814314}\n",
      "Iteration 15, Losses: {'ner': 17.85083773468035}\n",
      "Iteration 16, Losses: {'ner': 29.397530246748083}\n",
      "Iteration 17, Losses: {'ner': 25.588293585324184}\n",
      "Iteration 18, Losses: {'ner': 17.472247637660708}\n",
      "Iteration 19, Losses: {'ner': 16.6206080660219}\n",
      "Iteration 20, Losses: {'ner': 17.261017455079248}\n",
      "Iteration 21, Losses: {'ner': 25.930193057001894}\n",
      "Iteration 22, Losses: {'ner': 34.198073189674915}\n",
      "Iteration 23, Losses: {'ner': 23.332156751474184}\n",
      "Iteration 24, Losses: {'ner': 24.627552309716123}\n",
      "Iteration 25, Losses: {'ner': 10.764046509624915}\n",
      "Iteration 26, Losses: {'ner': 25.373553772957543}\n",
      "Iteration 27, Losses: {'ner': 25.11540958151469}\n",
      "Iteration 28, Losses: {'ner': 22.430292268271046}\n",
      "Iteration 29, Losses: {'ner': 16.838893562292757}\n",
      "Iteration 30, Losses: {'ner': 26.673109643548035}\n",
      "Iteration 31, Losses: {'ner': 8.905334486755907}\n",
      "Iteration 32, Losses: {'ner': 5.098312303031355}\n",
      "Iteration 33, Losses: {'ner': 18.11413936527767}\n",
      "Iteration 34, Losses: {'ner': 10.144461859980263}\n",
      "Iteration 35, Losses: {'ner': 11.884905149733246}\n",
      "Iteration 36, Losses: {'ner': 13.158588088982128}\n",
      "Iteration 37, Losses: {'ner': 34.586184823233765}\n",
      "Iteration 38, Losses: {'ner': 20.433495885370544}\n",
      "Iteration 39, Losses: {'ner': 20.465540915684613}\n",
      "Iteration 40, Losses: {'ner': 24.834602603787427}\n",
      "Model fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001  # Set desired learning rate\n",
    "\n",
    "# Disable other pipes except 'ner'\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.resume_training()\n",
    "    nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = learning_rate\n",
    "\n",
    "    # Fine-tune the model on the new dataset\n",
    "    for iteration in range(40):  # Maximum number of iterations\n",
    "        losses = {}\n",
    "        \n",
    "        # Create batches with the specified batch size\n",
    "        batches = spacy.util.minibatch(train_examples, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "        \n",
    "        # Update model in batches\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, sgd=optimizer, drop=0.3, losses=losses)\n",
    "        \n",
    "        # Display losses for the current iteration\n",
    "        print(f\"Iteration {iteration + 1}, Losses: {losses}\")\n",
    "\n",
    "        # Stop training if the total loss is below the threshold\n",
    "        total_loss = sum(losses.values())\n",
    "        if total_loss < 10:\n",
    "            print(\"Early stopping as loss is below threshold.\")\n",
    "            break\n",
    "\n",
    "    # Save the updated model\n",
    "    nlp.to_disk(\"0fner_model_custom_4_2\")\n",
    "\n",
    "print(\"Model fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aftershocks DISASTER\n",
      "M. Noto GPE\n",
      "earthquake DISASTER\n",
      "Japan GPE\n",
      "epicenter DISASTER\n",
      "Ishikawa GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "nlp_custom = spacy.load(\"0fner_model_custom_2_2\")\n",
    "\n",
    "# Test the model with a sample tweet\n",
    "doc = nlp_custom(\"The aftershocks pattern in purple suggests that the M. Noto earthquake ruptured bilaterally, with the preliminary NEIC slip model in Japan also indicating slip on both sides of the epicenter in Ishikawa\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration 1, Losses: {'ner': 1568.1188160713702}\n",
    "Iteration 2, Losses: {'ner': 459.29892484944276}\n",
    "Iteration 3, Losses: {'ner': 252.24542299692504}\n",
    "Iteration 4, Losses: {'ner': 138.23971964243464}\n",
    "Iteration 5, Losses: {'ner': 90.47280653886523}\n",
    "Iteration 6, Losses: {'ner': 62.04907696036728}\n",
    "Iteration 7, Losses: {'ner': 62.68806317947039}\n",
    "Iteration 8, Losses: {'ner': 42.515644518425944}\n",
    "Iteration 9, Losses: {'ner': 34.906144071241776}\n",
    "Iteration 10, Losses: {'ner': 44.153884356830304}\n",
    "Iteration 11, Losses: {'ner': 41.33845888993389}\n",
    "Iteration 12, Losses: {'ner': 37.745458430482884}\n",
    "Iteration 13, Losses: {'ner': 46.708330930778345}\n",
    "Iteration 14, Losses: {'ner': 27.025751817814314}\n",
    "Iteration 15, Losses: {'ner': 17.85083773468035}\n",
    "Iteration 16, Losses: {'ner': 29.397530246748083}\n",
    "Iteration 17, Losses: {'ner': 25.588293585324184}\n",
    "Iteration 18, Losses: {'ner': 17.472247637660708}\n",
    "Iteration 19, Losses: {'ner': 16.6206080660219}\n",
    "Iteration 20, Losses: {'ner': 17.261017455079248}\n",
    "Iteration 21, Losses: {'ner': 25.930193057001894}\n",
    "Iteration 22, Losses: {'ner': 34.198073189674915}\n",
    "Iteration 23, Losses: {'ner': 23.332156751474184}\n",
    "Iteration 24, Losses: {'ner': 24.627552309716123}\n",
    "Iteration 25, Losses: {'ner': 10.764046509624915}\n",
    "Iteration 26, Losses: {'ner': 25.373553772957543}\n",
    "Iteration 27, Losses: {'ner': 25.11540958151469}\n",
    "Iteration 28, Losses: {'ner': 22.430292268271046}\n",
    "Iteration 29, Losses: {'ner': 16.838893562292757}\n",
    "Iteration 30, Losses: {'ner': 26.673109643548035}\n",
    "Iteration 31, Losses: {'ner': 8.905334486755907}\n",
    "Iteration 32, Losses: {'ner': 5.098312303031355}\n",
    "Iteration 33, Losses: {'ner': 18.11413936527767}\n",
    "Iteration 34, Losses: {'ner': 10.144461859980263}\n",
    "Iteration 35, Losses: {'ner': 11.884905149733246}\n",
    "Iteration 36, Losses: {'ner': 13.158588088982128}\n",
    "Iteration 37, Losses: {'ner': 34.586184823233765}\n",
    "Iteration 38, Losses: {'ner': 20.433495885370544}\n",
    "Iteration 39, Losses: {'ner': 20.465540915684613}\n",
    "Iteration 40, Losses: {'ner': 24.834602603787427}\n",
    "Model fine-tuning complete!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
